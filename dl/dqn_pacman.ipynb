{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.array([210, 164, 74]).mean()\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    \n",
    "    img = obs[1:176:2, ::2]\n",
    "    img = img.mean(axis=2)\n",
    "    img[img==color] = 0\n",
    "    img = (img-128) / 128 - 1\n",
    "    return img.reshape(88,80,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "n_outputs = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def q_network(X, name_scope):\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    with tf.variable_scope(name_scope) as scope:\n",
    "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding=\"SAME\",weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_1', layer_1)\n",
    "        \n",
    "        layer_2 = conv2d(X, num_outputs=64, kernel_size=(4,4), stride=2, padding=\"SAME\",weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_2', layer_2)\n",
    "                \n",
    "        layer_3 = conv2d(X, num_outputs=64, kernel_size=(3,3), stride=1, padding=\"SAME\",weights_initializer=initializer)\n",
    "        tf.summary.histogram('layer_3', layer_3)\n",
    "        \n",
    "        flat = flatten(layer_3)\n",
    "        \n",
    "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
    "        tf.summary.histogram('fc',fc)\n",
    "        \n",
    "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "        tf.summary.histogram('output',output)\n",
    "        \n",
    "        vars = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}\n",
    "        return vars, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_min = 0.05\n",
    "eps_max = 0.5\n",
    "eps_decay_steps = 5000000\n",
    "\n",
    "def epsilon_greedy(action, step):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * steps/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49999991\n",
      "0.49909991\n",
      "0.49819991\n",
      "0.49729991\n",
      "0.49639991\n",
      "0.49549991\n",
      "0.49459991\n",
      "0.49369991\n",
      "0.49279991\n",
      "0.49189991\n",
      "0.49099991\n",
      "0.49009990999999997\n",
      "0.48919991\n",
      "0.48829991\n",
      "0.48739991\n",
      "0.48649991\n",
      "0.48559991\n",
      "0.48469991\n",
      "0.48379991\n",
      "0.48289991\n",
      "0.48199991\n",
      "0.48109991\n",
      "0.48019991\n",
      "0.47929991\n",
      "0.47839991\n",
      "0.47749991\n",
      "0.47659991\n",
      "0.47569991\n",
      "0.47479991\n",
      "0.47389991\n",
      "0.47299991\n",
      "0.47209991\n",
      "0.47119991\n",
      "0.47029991\n",
      "0.46939991\n",
      "0.46849991\n",
      "0.46759991\n",
      "0.46669991\n",
      "0.46579991\n",
      "0.46489990999999997\n",
      "0.46399991\n",
      "0.46309991\n",
      "0.46219991\n",
      "0.46129991\n",
      "0.46039991\n",
      "0.45949991\n",
      "0.45859991\n",
      "0.45769991\n",
      "0.45679991\n",
      "0.45589991\n",
      "0.45499991\n",
      "0.45409991\n",
      "0.45319991\n",
      "0.45229991\n",
      "0.45139991\n",
      "0.45049991\n",
      "0.44959991\n",
      "0.44869991\n",
      "0.44779990999999997\n",
      "0.44689991\n",
      "0.44599991\n",
      "0.44509991\n",
      "0.44419991\n",
      "0.44329991\n",
      "0.44239991\n",
      "0.44149991\n",
      "0.44059991\n",
      "0.43969990999999997\n",
      "0.43879991\n",
      "0.43789991\n",
      "0.43699991\n",
      "0.43609991\n",
      "0.43519991\n",
      "0.43429991\n",
      "0.43339991\n",
      "0.43249991\n",
      "0.43159991\n",
      "0.43069990999999996\n",
      "0.42979991\n",
      "0.42889991\n",
      "0.42799991\n",
      "0.42709991\n",
      "0.42619991\n",
      "0.42529991\n",
      "0.42439991\n",
      "0.42349991\n",
      "0.42259990999999997\n",
      "0.42169991\n",
      "0.42079991\n",
      "0.41989991\n",
      "0.41899991000000003\n",
      "0.41809991\n",
      "0.41719991\n",
      "0.41629991\n",
      "0.41539991\n",
      "0.41449990999999997\n",
      "0.41359991\n",
      "0.41269991\n",
      "0.41179991\n",
      "0.41089991\n",
      "0.40999991\n",
      "0.40909991\n",
      "0.40819991\n",
      "0.40729991\n",
      "0.40639991\n",
      "0.40549990999999996\n",
      "0.40459991\n",
      "0.40369991\n",
      "0.40279991\n",
      "0.40189991\n",
      "0.40099991\n",
      "0.40009991\n",
      "0.39919991\n",
      "0.39829991\n",
      "0.39739990999999997\n",
      "0.39649991\n",
      "0.39559991\n",
      "0.39469991\n",
      "0.39379991\n",
      "0.39289991\n",
      "0.39199991\n",
      "0.39109991\n",
      "0.39019991\n",
      "0.38929990999999997\n",
      "0.38839990999999996\n",
      "0.38749991\n",
      "0.38659991\n",
      "0.38569991\n",
      "0.38479990999999997\n",
      "0.38389991\n",
      "0.38299991\n",
      "0.38209991\n",
      "0.38119991\n",
      "0.38029990999999996\n",
      "0.37939990999999995\n",
      "0.37849991\n",
      "0.37759991\n",
      "0.37669990999999997\n",
      "0.37579991\n",
      "0.37489991\n",
      "0.37399991\n",
      "0.37309991\n",
      "0.37219990999999997\n",
      "0.37129990999999996\n",
      "0.37039991\n",
      "0.36949991\n",
      "0.36859991\n",
      "0.36769991\n",
      "0.36679991\n",
      "0.36589991\n",
      "0.36499991\n",
      "0.36409990999999997\n",
      "0.36319990999999996\n",
      "0.36229990999999995\n",
      "0.36139991\n",
      "0.36049991\n",
      "0.35959991\n",
      "0.35869991\n",
      "0.35779991\n",
      "0.35689991\n",
      "0.35599991\n",
      "0.35509990999999996\n",
      "0.35419990999999995\n",
      "0.35329991\n",
      "0.35239991\n",
      "0.35149991\n",
      "0.35059991\n",
      "0.34969991\n",
      "0.34879991\n",
      "0.34789991\n",
      "0.34699990999999997\n",
      "0.34609990999999996\n",
      "0.34519991\n",
      "0.34429991\n",
      "0.34339991\n",
      "0.34249991\n",
      "0.34159991\n",
      "0.34069991\n",
      "0.33979991\n",
      "0.33889990999999997\n",
      "0.33799990999999996\n",
      "0.33709990999999995\n",
      "0.33619991\n",
      "0.33529991\n",
      "0.33439991\n",
      "0.33349991\n",
      "0.33259991\n",
      "0.33169991\n",
      "0.33079991\n",
      "0.32989990999999996\n",
      "0.32899990999999995\n",
      "0.32809991\n",
      "0.32719991\n",
      "0.32629991\n",
      "0.32539991\n",
      "0.32449991\n",
      "0.32359991\n",
      "0.32269991\n",
      "0.32179990999999997\n",
      "0.32089990999999995\n",
      "0.31999991\n",
      "0.31909991\n",
      "0.31819991\n",
      "0.31729991\n",
      "0.31639991\n",
      "0.31549991\n",
      "0.31459991\n",
      "0.31369990999999997\n",
      "0.31279990999999996\n",
      "0.31189990999999995\n",
      "0.31099991\n",
      "0.31009991\n",
      "0.30919991\n",
      "0.30829991\n",
      "0.30739991\n",
      "0.30649991\n",
      "0.30559991\n",
      "0.30469990999999996\n",
      "0.30379990999999995\n",
      "0.30289991\n",
      "0.30199991\n",
      "0.30109991\n",
      "0.30019991\n",
      "0.29929991\n",
      "0.29839991\n",
      "0.29749991\n",
      "0.29659990999999997\n",
      "0.29569990999999995\n",
      "0.29479991\n",
      "0.29389991\n",
      "0.29299991\n",
      "0.29209991\n",
      "0.29119991\n",
      "0.29029991\n",
      "0.28939991\n",
      "0.28849990999999997\n",
      "0.28759991\n",
      "0.28669991\n",
      "0.28579991000000005\n",
      "0.28489991000000003\n",
      "0.28399991\n",
      "0.28309991\n",
      "0.28219991\n",
      "0.28129991\n",
      "0.28039991\n",
      "0.27949991\n",
      "0.27859991\n",
      "0.27769991000000005\n",
      "0.27679991000000004\n",
      "0.27589991\n",
      "0.27499991\n",
      "0.27409991\n",
      "0.27319991\n",
      "0.27229991\n",
      "0.27139991\n",
      "0.27049991\n",
      "0.26959991\n",
      "0.26869991000000004\n",
      "0.26779991000000003\n",
      "0.26689991\n",
      "0.26599991\n",
      "0.26509991\n",
      "0.26419991\n",
      "0.26329990999999997\n",
      "0.26239991\n",
      "0.26149991\n",
      "0.26059991000000005\n",
      "0.25969991000000003\n",
      "0.25879991\n",
      "0.25789991\n",
      "0.25699991\n",
      "0.25609991\n",
      "0.25519991\n",
      "0.25429991\n",
      "0.25339991\n",
      "0.25249991000000005\n",
      "0.25159991000000004\n",
      "0.25069991\n",
      "0.24979991000000001\n",
      "0.24889991\n",
      "0.24799991\n",
      "0.24709991000000003\n",
      "0.24619991000000002\n",
      "0.24529991\n",
      "0.24439991\n",
      "0.24349990999999999\n",
      "0.24259991000000003\n",
      "0.24169991000000002\n",
      "0.24079991\n",
      "0.23989991\n",
      "0.23899990999999998\n",
      "0.23809991000000003\n",
      "0.23719991\n",
      "0.23629991\n",
      "0.23539991\n",
      "0.23449991000000003\n",
      "0.23359991000000002\n",
      "0.23269991\n",
      "0.23179991\n",
      "0.23089990999999999\n",
      "0.22999991000000003\n",
      "0.22909991000000002\n",
      "0.22819991\n",
      "0.22729991\n",
      "0.22639990999999998\n",
      "0.22549991000000003\n",
      "0.22459991\n",
      "0.22369991\n",
      "0.22279991\n",
      "0.22189991000000003\n",
      "0.22099991000000002\n",
      "0.22009991\n",
      "0.21919991\n",
      "0.21829990999999999\n",
      "0.21739991000000003\n",
      "0.21649991000000002\n",
      "0.21559991\n",
      "0.21469991\n",
      "0.21379990999999998\n",
      "0.21289991000000003\n",
      "0.21199991\n",
      "0.21109991\n",
      "0.21019991\n",
      "0.20929991000000003\n",
      "0.20839991000000002\n",
      "0.20749991\n",
      "0.20659991\n",
      "0.20569990999999999\n",
      "0.20479991000000003\n",
      "0.20389991000000002\n",
      "0.20299991\n",
      "0.20209991\n",
      "0.20119990999999998\n",
      "0.20029991000000003\n",
      "0.19939991\n",
      "0.19849991\n",
      "0.19759991\n",
      "0.19669991000000003\n",
      "0.19579991000000002\n",
      "0.19489991\n",
      "0.19399991\n",
      "0.19309990999999999\n",
      "0.19219991000000003\n",
      "0.19129991000000002\n",
      "0.19039991\n",
      "0.18949991\n",
      "0.18859990999999998\n",
      "0.18769991000000003\n",
      "0.18679991\n",
      "0.18589991\n",
      "0.18499991\n",
      "0.18409991000000003\n",
      "0.18319991000000002\n",
      "0.18229991\n",
      "0.18139991\n",
      "0.18049990999999999\n",
      "0.17959991000000003\n",
      "0.17869991000000002\n",
      "0.17779991\n",
      "0.17689991\n",
      "0.17599990999999998\n",
      "0.17509991000000003\n",
      "0.17419991\n",
      "0.17329991\n",
      "0.17239991\n",
      "0.17149991000000003\n",
      "0.17059991000000002\n",
      "0.16969991\n",
      "0.16879991\n",
      "0.16789990999999999\n",
      "0.16699991000000003\n",
      "0.16609991000000002\n",
      "0.16519991\n",
      "0.16429991\n",
      "0.16339990999999998\n",
      "0.16249991000000003\n",
      "0.16159991\n",
      "0.16069991\n",
      "0.15979991\n",
      "0.15889991000000003\n",
      "0.15799991000000002\n",
      "0.15709991\n",
      "0.15619991\n",
      "0.15529990999999999\n",
      "0.15439991000000003\n",
      "0.15349991000000002\n",
      "0.15259991\n",
      "0.15169991\n",
      "0.15079991000000004\n",
      "0.14989991000000003\n",
      "0.14899991\n",
      "0.14809991\n",
      "0.14719991\n",
      "0.14629991000000003\n",
      "0.14539991000000002\n",
      "0.14449991\n",
      "0.14359991\n",
      "0.14269990999999999\n",
      "0.14179991000000003\n",
      "0.14089991000000002\n",
      "0.13999991\n",
      "0.13909991\n",
      "0.13819991000000004\n",
      "0.13729991000000002\n",
      "0.13639991\n",
      "0.13549991\n",
      "0.13459991\n",
      "0.13369991000000003\n",
      "0.13279991000000002\n",
      "0.13189991\n",
      "0.13099991\n",
      "0.13009990999999999\n",
      "0.12919991000000003\n",
      "0.12829991000000002\n",
      "0.12739991\n",
      "0.12649991\n",
      "0.12559991000000004\n",
      "0.12469991000000002\n",
      "0.12379991000000001\n",
      "0.12289991\n",
      "0.12199990999999999\n",
      "0.12109991000000003\n",
      "0.12019991000000002\n",
      "0.11929991000000001\n",
      "0.11839991\n",
      "0.11749990999999999\n",
      "0.11659991000000003\n",
      "0.11569991000000002\n",
      "0.11479991\n",
      "0.11389991\n",
      "0.11299991000000004\n",
      "0.11209991000000002\n",
      "0.11119991000000001\n",
      "0.11029991\n",
      "0.10939990999999999\n",
      "0.10849991000000003\n",
      "0.10759991000000002\n",
      "0.10669991000000001\n",
      "0.10579991\n",
      "0.10489990999999999\n",
      "0.10399991000000003\n",
      "0.10309991000000002\n",
      "0.10219991\n",
      "0.10129990999999999\n",
      "0.10039991000000004\n",
      "0.09949991000000002\n",
      "0.09859991000000001\n",
      "0.09769991\n",
      "0.09679990999999999\n",
      "0.09589991000000003\n",
      "0.09499991000000002\n",
      "0.09409991000000001\n",
      "0.09319991\n",
      "0.09229990999999999\n",
      "0.09139991000000003\n",
      "0.09049991000000002\n",
      "0.08959991\n",
      "0.08869990999999999\n",
      "0.08779991000000004\n",
      "0.08689991000000002\n",
      "0.08599991000000001\n",
      "0.08509991\n",
      "0.08419990999999999\n",
      "0.08329991000000003\n",
      "0.08239991000000002\n",
      "0.08149991000000001\n",
      "0.08059991\n",
      "0.07969990999999998\n",
      "0.07879990999999997\n",
      "0.07789990999999996\n",
      "0.07699990999999995\n",
      "0.07609990999999994\n",
      "0.07519990999999998\n",
      "0.07429990999999997\n",
      "0.07339990999999996\n",
      "0.07249990999999995\n",
      "0.07159990999999999\n",
      "0.07069990999999998\n",
      "0.06979990999999997\n",
      "0.06889990999999995\n",
      "0.06799990999999994\n",
      "0.06709990999999998\n",
      "0.06619990999999997\n",
      "0.06529990999999996\n",
      "0.06439990999999995\n",
      "0.06349990999999994\n",
      "0.06259990999999998\n",
      "0.06169990999999997\n",
      "0.06079990999999996\n",
      "0.059899909999999945\n",
      "0.05899990999999999\n",
      "0.05809990999999998\n",
      "0.057199909999999965\n",
      "0.05629990999999995\n",
      "0.05539990999999994\n",
      "0.054499909999999985\n",
      "0.05359990999999997\n",
      "0.05269990999999996\n",
      "0.05179990999999995\n",
      "0.05089990999999994\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "for steps in range(1, 10000000, 10000):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * steps/eps_decay_steps)\n",
    "    print(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_len = 20000\n",
    "exp_buffer = deque(maxlen=buffer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
    "    mem = np.array(exp_buffer)[perm_batch]\n",
    "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 800\n",
    "batch_size = 48\n",
    "input_shape = (None, 88, 80, 1)\n",
    "learning_rate = 0.001\n",
    "X_shape = (None, 88, 80, 1)\n",
    "discount_factor = 0.97\n",
    "\n",
    "global_step = 0\n",
    "copy_steps = 100\n",
    "steps_train = 4\n",
    "start_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'ch8_logs'\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=X_shape)\n",
    "in_training_mode = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
    "\n",
    "targetQ, targetQ_outputs = q_network(X, 'targetQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_action = tf.placeholder(tf.int32, shape=(None,))\n",
    "Q_action = tf.reduce_sum(targetQ_outputs * tf.one_hot(X_action, n_outputs),axis=-1,keep_dims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_op = [tf.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
    "copy_target_to_main = tf.group(*copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y-Q_action))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_summary = tf.summary.scalar('LOSS', loss)\n",
    "merge_summary = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        epoch = 0\n",
    "        episodic_reward = 0\n",
    "        actions_counter = Counter()\n",
    "        episodic_loss = []\n",
    "        \n",
    "        while not done:\n",
    "            obs = preprocess_observation(obs)\n",
    "            \n",
    "            # 현재 상태에서의 Q-값을 계산\n",
    "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False}) \n",
    "            \n",
    "            action = np.argmax(actions, axis=-1)\n",
    "            actions_counter[str(action)] += 1\n",
    "            action = epsilon_greedy(action, global_step)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # 현재 상태, 행동, 다음 상태, 보상, 종료 여부를 buffer에 저장\n",
    "            exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])\n",
    "            \n",
    "            if global_step % steps_train == 0 and global_step > start_steps:\n",
    "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
    "                o_obs = [x for x in o_obs]\n",
    "                o_next_obs = [x for x in o_next_obs]\n",
    "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
    "                \n",
    "                # Q(s,a)←Q(s,a)+α[r+γmax(Q(s′,a′))−Q(s,a)]\n",
    "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)\n",
    "                \n",
    "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
    "                file_writer.add_summary(mrg_summary, global_step)\n",
    "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_actions:o_act, in_training_mode:True})\n",
    "                episodic_loss.append(train_loss)\n",
    "            \n",
    "            # copy main Q network weights to target Q networks after some intervals\n",
    "            if (global_step+1)%copy_steps==0 and global_step > start_steps:\n",
    "                copy_target_to_main.run()\n",
    "                \n",
    "            obs = next_obs\n",
    "            epoch += 1\n",
    "            global_step += 1\n",
    "            episodic_reward += reward\n",
    "            \n",
    "        print('Epoch', epoch, 'Reward', episodic_reward,)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
